# LFM2-localAI
Quick chat interface with LFM2-700M and llama.cpp server


Here's the best part: LFM2 is open source . That means you can download it and try it yourself.

Here I created a useful (and easy) GitHub repository to fully automate (on Windows) the installation and run of a slim chat application powered by llama.cpp server and LFM2–700M.gguf.

How you can do it yourself?

Download on a local folder my install.bat file. My test folder is called LFM2. Open in the terminal command line and run it…

Learn more about [Liquid.ai models here](https://fabiomatricardi.github.io/LFM2-localAI/)

Page created with Gemini
