# LFM2-localAI
Quick chat interface with LFM2-700M and llama.cpp server

<img src='https://github.com/fabiomatricardi/LFM2-localAI/raw/main/LFM2.gif' width=1000>

Here's the best part: LFM2 is open sourceÂ . That means you can download it and try it yourself.

Here I created a useful (and easy) GitHub repository to fully automate (on Windows) the installation and run of a slim chat application powered by llama.cpp server and LFM2â€“700M.gguf.

How you can do it yourself?

Download on a local folder my install.bat file. My test folder is called LFM2. Open in the terminal command line and run itâ€¦


The file will:
- download wget.exe
- download the llama.cpp binaries (version b5943)
- download the model GGUF weights (LFM2â€“700M-Q8_0.gguf)
- download a sever.bat file (for future runs)
- Unzip the llama.cpp binaries
- start the server on your computer

> NOTE on the model: the 700M variant is perfect for normal chats, but do not ask too much recent questions! It is really good for summarization, topics and question answering.

> PS: watch the GIF until the end ðŸ¤£
>
>  **I tried again with the 1.2b GGUF variant and it is much betterâ€¦**


---


## Interactive LFM model card

Learn more about [Liquid.ai models here](https://fabiomatricardi.github.io/LFM2-localAI/)

Page created with Gemini

---

