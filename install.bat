@echo off
echo Downloading the wget binaries...
curl -L -o wget.exe https://github.com/fabiomatricardi/VisualAI-4ALL/raw/main/wget.exe
echo Downloading the LlamaCPP binaries...
wget.exe https://github.com/ggml-org/llama.cpp/releases/download/b5943/llama-b5943-bin-win-cpu-x64.zip -nv --show-progress
wget.exe https://github.com/fabiomatricardi/LFM2-localAI/raw/main/server.bat -nv --show-progress
echo Downloading the Language model...
wget https://hf-mirror.com/LiquidAI/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q8_0.gguf -nv --show-progress
echo Unzipping the llama.cpp binaries...
tar -xf llama-b5943-bin-win-cpu-x64.zip
echo Starting llama-server with 8192 tokens context window...
start cmd.exe /c llama-server.exe -m LFM2-700M-Q8_0.gguf
echo Opening default browser in 15 seconds...
timeout /t 15
start http://localhost:8080
